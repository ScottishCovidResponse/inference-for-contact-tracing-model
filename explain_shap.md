# SHAP to Interprete the Analysis Results

By running `src/analyse_parameters.py` we obtain multiple scatter plots to understand
the relationship between input parameters and output by `Contact-Tracing-Model`.
These plots are provided by [SHapley Additive exPlanations (SHAP)](https://github.com/slundberg/shap),
and in this document we explain how to read the resulting PDF files. 
The essential formulas of SHAP first appeared in
the [NeurIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions),
while here we provide a simplified explanation with omitting proofs etc.
Readers who are already familiar with SHAP can directly proceed into [this document](./read_plots.md).

## Local Sensitivity in Non-Linear Regression

Let <img src="https://render.githubusercontent.com/render/math?math=X\in{\mathbb R}^d"> be a random variable
to represent the vector of free input parameters (e.g., `random_infection_rate` and `exposureExponent`) whose dimensionality (number of parameters) is <img src="https://render.githubusercontent.com/render/math?math=d">. 
For the output (e.g., SEIR) time-series generated by `Contact-Tracing-Model`, let
<img src="https://render.githubusercontent.com/render/math?math=Y\in{\mathbb R}">
be a focused metric that can be computed from that output time-series.
Because `Contact-Tracing-Model` is a stochastic system to generate <img src="https://render.githubusercontent.com/render/math?math=Y"> from <img src="https://render.githubusercontent.com/render/math?math=X">,
there should be a function <img src="https://render.githubusercontent.com/render/math?math=f: {\mathbb R}^d\to{\mathbb R}">
such that <img src="https://render.githubusercontent.com/render/math?math=Y= f(X) %2B \varepsilon"> with a zero-mean
random noise variable <img src="https://render.githubusercontent.com/render/math?math=\varepsilon">.

Suppose that some machine learning algorithm supplies a good regression function <img src="https://render.githubusercontent.com/render/math?math=\widehat{f}: {\mathbb R}^d\to{\mathbb R}"> that well approximates
<img src="https://render.githubusercontent.com/render/math?math=f">.
We adopt [Extremely Randomised Trees](https://link.springer.com/article/10.1007/s10994-006-6226-1)
as <img src="https://render.githubusercontent.com/render/math?math=\widehat{f}"> in the current implementation, 
but users can switch into a more sophisticated predictor such as [XGBoost](https://github.com/dmlc/xgboost), [LightGBM](https://github.com/microsoft/LightGBM), or [CatBoost](https://catboost.ai/), 
as long as the fitted predictor is a good approximator of the whole system. 
We encourage fine-tuning of the hyperparameters by cross-validation.

Because <img src="https://render.githubusercontent.com/render/math?math=\widehat{f}"> is non-linear,
sensitivity from each input parameter to the output metric is not perfectly described as a single scalar.
By contrast, in linear regression, sensitivity is simply supplied as regression coefficient that means 
the change of output per unit change of input.
This interpretation of output change by perturbing input is also useful even in non-linear regression,
if we calibrate *locally* at each data point.

Let <img src="https://render.githubusercontent.com/render/math?math=(\boldsymbol{x}_t, y_t)_{t=1}^n">
be samples of input and output and assume that we already have the function <img src="https://render.githubusercontent.com/render/math?math=\widehat{f}">.
For each input point <img src="https://render.githubusercontent.com/render/math?math=\boldsymbol{x}_t">,
we can obtain a perturbed input as <img src="https://render.githubusercontent.com/render/math?math=\boldsymbol{x}_t'=\boldsymbol{x}_t %2B \boldsymbol{\varepsilon}_t">. Then
we can compute how the expected output changes, as 
<img src="https://render.githubusercontent.com/render/math?math=\Delta \widehat{f}_t\triangleq\widehat{f}(\boldsymbol{x}_t %2B \boldsymbol{\varepsilon}_t)-\widehat{f}(\boldsymbol{x}_t)">.
If the perturbation is specific to a chosen parameter as <img src="https://render.githubusercontent.com/render/math?math=\boldsymbol{\varepsilon}_t=(0,\ldots,0,\underbrace{\varepsilon_t}_{i},0,\ldots,0)^\top">
then the numerical derivative
<img src="https://render.githubusercontent.com/render/math?math=\Delta \widehat{f}_t / \varepsilon_t">
corresponds to an approximate local regression coefficient of the <img src="https://render.githubusercontent.com/render/math?math=i">-th input variable. The distribution
of that coefficient across all inputs <img src="https://render.githubusercontent.com/render/math?math=(\boldsymbol{x}_t)_{t=1}^n"> clarifies how the output is sensitive to the input in general, and how that magnitude is varying
inside the input space, as long as the empirical input distribution well covers the entire support of input space.

## SHAP as the Local Sensitivity by Enabling an Input Variable

While the idea of empirically calibrating local-linear regression coefficients looks sensible,
that direction is not perfect because that approximation works only within the small radius around the chosen input point.
Also the regression function 
<img src="https://render.githubusercontent.com/render/math?math=\widehat{f}">.
is technically non-differentiable for some types of regressors such as trees.

Another way of obtaining a reasonable distribution of local sensitivities is to calibrate
the output change when *enabling or completely disabling* the specifically chosen input variable. For example,
for the original input sample
<img src="https://render.githubusercontent.com/render/math?math=\boldsymbol{x}_t=(x_{t1},\ldots,x_{td})^\top">,
we can consider a variation
<img src="https://render.githubusercontent.com/render/math?math=\boldsymbol{x}_t^{\setminus i}=(x_{t1},\ldots,x_{t(i-1)},0,x_{t(i %2B 1)},\ldots,x_{td})^\top"> that *spoils* the predictability by the <img src="https://render.githubusercontent.com/render/math?math=i">-th input variable. Note that replacement value does not have
to be zero. As long as this value is the same across all samples, we can disable the input variable specifically.
Then we could calibrate the output change as
<img src="https://render.githubusercontent.com/render/math?math=\widehat{f}(\boldsymbol{x}_t)-\widehat{f}(\boldsymbol{x}_t^{\setminus i})">. If this value is large positive, then we could say that <img src="https://render.githubusercontent.com/render/math?math=i">-th input variable has a large contribution to the output
and hence is an important parameter in our simulation model.

The magnitude of output change, however, can depend on which other variables are included
in the evaluation. For example, instead of the difference between all and all-but-one, one could consider
the difference between two variables and one variable. A natural way to obtain the sensitivity
is to use the average the output change across all possible subset of the input variables. 
Let <img src="https://render.githubusercontent.com/render/math?math={\mathcal F}\triangleq\lbrace 1,2,\ldots,d\rbrace">
be the set of all input variables
and <img src="https://render.githubusercontent.com/render/math?math=\widehat{f}_{{\mathcal S}}(\boldsymbol{x})=\widehat{f}(\boldsymbol{x}_{{\mathcal S}})"> where <img src="https://render.githubusercontent.com/render/math?math=\boldsymbol{x}_{{\mathcal S}}"> is obtained by zeroing the input variables of
<img src="https://render.githubusercontent.com/render/math?math=\boldsymbol{x}"> other than those included in set 
<img src="https://render.githubusercontent.com/render/math?math={\mathcal S}">. Then we define

<img src="https://render.githubusercontent.com/render/math?math=\phi_i(\boldsymbol{x}_t)=\sum_{{\mathcal S}\subseteq{\mathcal F}}\dfrac{\vert{\mathcal S}\vert!(\vert{\mathcal F}\vert-\vert{\mathcal S}\vert-1)!}{\vert{\mathcal F}\vert!}\left[\widehat{f}_{{\mathcal S}\cup\lbrace i \rbrace}(\boldsymbol{x}_t)-\widehat{f}_{\mathcal S}(\boldsymbol{x}_t)\right]">

as the SHAP value of <img src="https://render.githubusercontent.com/render/math?math=i">-th input variable
at sample 
<img src="https://render.githubusercontent.com/render/math?math=\boldsymbol{x}_t">.
The name of SHAP value stems from the classic Shapley regression values,
and this definition satisfies the three important properties in local attribution methods: local accuracy,
missingness, and consistency. Local accuracy means that at each local data point, its corresponding output
of the regression function always equals to a local linear model whose coefficients are the SHAP values themselves.
Missingness means the constraint that
Consistency means that 
if contribution by adding one variable is larger in one regression model than in the other model
for any subset of input variables, then the SHAP value by the former regression model
is always larger than the latter model. For details, refer to the 
[NeurIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions).
 
In practice SHAP values work as a sensible extension of local-linear regression coefficients
that can work with any regression function in machine learning. It tells us whether
input variable has positive or negative impact to the output depending on each local data point.
Because it is a datapoint-specific coefficient we can also evaluate which other variable is most influential
to change the relationship between the input variable and output.  
