# How to Read the Analysis Results

By running `src/analyse_parameters.py` we obtain multiple scatter plots to understand
the relationship between input parameters and output by `Contact-Tracing-Model`.
These plots are provided by [SHAP](https://github.com/slundberg/shap) and in this document
we explain how to read the resulting PDF files.

# What is SHAP

We first excerpt the essential formulas that first appeared in
the [NeurIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions).

Let ![formula](https://render.githubusercontent.com/render/math?math=X\in{\mathbb R}^d) be a random variable
to represent the vector of free input parameters (e.g., `random_infection_rate` and `exposureExponent`) whose dimensionality (number of parameters) is ![formula](https://render.githubusercontent.com/render/math?math=d). 
For the output (e.g., SEIR) time-series generated by `Contact-Tracing-Model`, let
![formula](https://render.githubusercontent.com/render/math?math=Y\in{\mathbb R})
be a focused metric that can be computed from that output time-series.
Because `Contact-Tracing-Model` is a stochastic system to generate ![formula](https://render.githubusercontent.com/render/math?math=Y) from ![formula](https://render.githubusercontent.com/render/math?math=X),
there should be a function ![formula](https://render.githubusercontent.com/render/math?math=f: {\mathbb R}^d\to{\mathbb R})
such that ![formula](https://render.githubusercontent.com/render/math?math=Y=f(X)+\varepsilon), where
![formula](https://render.githubusercontent.com/render/math?math=\varepsilon) is a random noise whose mean is zero.

Suppose that some machine learning algorithm supplies a good regression function ![formula](https://render.githubusercontent.com/render/math?math=\widehat{f}: {\mathbb R}^d\to{\mathbb R}) that well approximates
![formula](https://render.githubusercontent.com/render/math?math=f).
We adopt [Extremely Randomised Trees](https://link.springer.com/article/10.1007/s10994-006-6226-1)
as ![formula](https://render.githubusercontent.com/render/math?math=\widehat{f}) in the current implementation, 
but users can switch into a more sophisticated predictor such as [XGBoost](https://github.com/dmlc/xgboost), [LightGBM](https://github.com/microsoft/LightGBM), or [CatBoost](https://catboost.ai/), 
as long as the fitted predictor is a good approximator of the whole system. 
We encourage fine-tuning of the hyperparameters by cross-validation.

Because ![formula](https://render.githubusercontent.com/render/math?math=\widehat{f}) is non-linear,
we cannot perfectly represent the sensitivity from each input parameter to the output metric
by a single scalar, such as linear regression coefficient that means the change of output by unit change of input.
The idea of perturbing input and watching the change of output, however, is still useful if we exploit it
*locally* at each data point. We here think of

 



 
